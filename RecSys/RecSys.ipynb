{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install cufflinks==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os\n",
    "import re, json\n",
    "import pickle, gzip\n",
    "import itertools\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "import boto3\n",
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "import cufflinks as cf\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import  Image\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# plotly + cufflinks work offline\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'slalom-ml'\n",
    "prefix = 'tmp/sagemaker/demo/recsys/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Movie Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "!unzip -o ml-100k.zip\n",
    "%cd ml-100k\n",
    "#!shuf ua.base -o ua.base.shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Data and Exploratory Data Analysis\n",
    "\n",
    "We observe the ratings are not in a matrix format, but are in a _long and skinny_ format.  We'll need to build the matrix ourselves.\n",
    "\n",
    "We also observe there is a user dataset in **u.user**, providing some infor about gender, occupation, and zipcode. And information about the movie itself: title, release date, URL, and category in the **u.item** file.  Lastly, I think about how I've rated movies; I'm curious if there is any skew to the ratings themselves.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_column_names = ['user_id', 'age', 'gender', 'occupation', 'zip code']\n",
    "film_column_names = ['film_id', 'title', 'release date', 'home release date', 'URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Noir','Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "data_column_names = ['user_id', 'film_id', 'rating', 'timestamp']\n",
    "user_df = pd.read_csv('u.user', sep='|', names=user_column_names)\n",
    "film_df = pd.read_csv('u.item', sep='|', names=film_column_names, encoding = \"ISO-8859-1\")\n",
    "\n",
    "ua_data = pd.read_csv('ua.base', sep='\\t', names=data_column_names).drop(['timestamp'], axis=1)\n",
    "ua_test = pd.read_csv('ua.test', sep='\\t', names=data_column_names).drop(['timestamp'], axis=1)\n",
    "\n",
    "data_df = ua_data\n",
    "test_df = ua_test\n",
    "print('\\nDESCRIPTION of Ratings data\\n')\n",
    "print(data_df.describe())\n",
    "\n",
    "print('\\n\\nSAMPLE of UA Training (ratings)  data\\n')\n",
    "print(data_df.sample(n=5))\n",
    "\n",
    "print('\\n\\nSAMPLE of UA Testing (ratings) data\\n')\n",
    "print(test_df.sample(n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = data_df.user_id.max()\n",
    "films = data_df.film_id.max()\n",
    "\n",
    "details = pd.DataFrame({'users': [users], 'films': [films]})\n",
    "details.iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "Is there a lot of skew in our data?  What does the rating distribution look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.groupby('rating').count()['film_id'].iplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tmp = data_df.groupby('user_id').count()\n",
    "tmp = tmp.rename(columns={'film_id' : 'film_count'})\n",
    "tmp.groupby('film_count').size().iplot(kind='bar')\n",
    "#tmp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and populate matrix for Matrix-Factorization\n",
    "We observe our dataset has 1682 films rated by 943 users. That will be the size of our matrix. We also want to know about the sparsity of our matrix; so we'll calculate that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building UA Training Matrix\n",
      "Sparsity: 5.71%\n",
      "Building UA Testing Matrix\n",
      "Sparsity: 0.59%\n"
     ]
    }
   ],
   "source": [
    "def build_matrix(user_max, film_max, df, name=''):\n",
    "    print('Building {name} Matrix'.format(name=name))\n",
    "\n",
    "    matrix = np.zeros((user_max, film_max))\n",
    "    for row in df.itertuples():\n",
    "        matrix[row.user_id - 1, row.film_id - 1] = row.rating\n",
    "        \n",
    "    sparsity = float(len(matrix.nonzero()[0]))\n",
    "    sparsity /= (matrix.shape[0] * matrix.shape[1])\n",
    "    sparsity *= 100\n",
    "    print('Sparsity: {:4.2f}%'.format(sparsity))\n",
    "    return matrix\n",
    "\n",
    "training_matrix = build_matrix(users, films, data_df, name='UA Training')\n",
    "testing_matrix  = build_matrix(users, films, test_df, name='UA Testing')\n",
    "\n",
    "# Validate we have a disjoint training/testing datasets\n",
    "assert(np.all((training_matrix * testing_matrix) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://gist.github.com/EthanRosenthal/a0816d8fea4394baf732\n",
    "from numpy.linalg import solve\n",
    "\n",
    "class ExplicitMF():\n",
    "    def __init__(self, ratings, iterations=[10], n_factors=40, item_reg=0.0, user_reg=0.0, verbose=False):\n",
    "        \"\"\"\n",
    "        Train a matrix factorization model to predict all empty entries in a matrix.\n",
    "        The terminology assumes a ratings matrix which is ~ USER x ITEM\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        ratings : (ndarray)\n",
    "            User x Item matrix with corresponding ratings\n",
    "        \n",
    "        n_factors : (int)\n",
    "            Number of latent factors (to assume) in factorization model\n",
    "        \n",
    "        item_reg : (float)\n",
    "            Regularization term for item latent factors\n",
    "        \n",
    "        user_reg : (float)\n",
    "            Regularization term for user latent factors\n",
    "        \n",
    "        verbose : (bool)\n",
    "            Whether or not to printout training progress\n",
    "        \"\"\"\n",
    "        \n",
    "        self.ratings = ratings\n",
    "        self.n_users, self.n_items = ratings.shape\n",
    "        self.n_factors = n_factors\n",
    "        self.item_reg = item_reg\n",
    "        self.user_reg = user_reg\n",
    "        self.iterations = iterations\n",
    "        self._v = verbose\n",
    "\n",
    "\n",
    "    def als_step(self, latent_vectors, fixed_vecs, ratings, _lambda, type='user'):\n",
    "        \"\"\" One of two ALS steps. Solve for the latent vectors specified by type. \"\"\"\n",
    "\n",
    "        if type == 'user':\n",
    "            # Precompute\n",
    "            YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(YTY.shape[0]) * _lambda\n",
    "            for u in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[u, :] = solve((YTY + lambdaI), ratings[u, :].dot(fixed_vecs))\n",
    "        \n",
    "        elif type == 'item':\n",
    "            # Precompute\n",
    "            XTX = fixed_vecs.T.dot(fixed_vecs)\n",
    "            lambdaI = np.eye(XTX.shape[0]) * _lambda\n",
    "            for i in range(latent_vectors.shape[0]):\n",
    "                latent_vectors[i, :] = solve((XTX + lambdaI), ratings[:, i].T.dot(fixed_vecs))\n",
    "\n",
    "        return latent_vectors\n",
    "\n",
    "    \n",
    "    \n",
    "    def train(self, n_iter = 10):\n",
    "        \"\"\" Train model for n_iter iterations from scratch.\"\"\"\n",
    "        # initialize latent vectors\n",
    "        self.user_vecs = np.random.random((self.n_users, self.n_factors))\n",
    "        self.item_vecs = np.random.random((self.n_items, self.n_factors))        \n",
    "        self.partial_train(n_iter)\n",
    "\n",
    "        \n",
    "    \n",
    "    def partial_train(self, n_iter):\n",
    "        \"\"\" Train model for n_iter iterations. Can be called multiple times for further training. \"\"\"\n",
    "        while (n_iter):\n",
    "            if (self._v): print('\\titerations left: {}'.format(n_iter))\n",
    "            self.user_vecs = self.als_step(self.user_vecs, self.item_vecs, self.ratings, self.user_reg, type='user')\n",
    "            self.item_vecs = self.als_step(self.item_vecs, self.user_vecs, self.ratings, self.item_reg, type='item')\n",
    "            n_iter = n_iter - 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_all(self):\n",
    "        \"\"\" Predict ratings for every user and item. \"\"\"\n",
    "        predictions = np.zeros((self.user_vecs.shape[0], self.item_vecs.shape[0]))\n",
    "        for u in range(self.user_vecs.shape[0]):\n",
    "            for i in range(self.item_vecs.shape[0]):\n",
    "                predictions[u, i] = self.predict(u, i)\n",
    "                \n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict(self, u, i):\n",
    "        \"\"\" Single user and item prediction. \"\"\"\n",
    "        return self.user_vecs[u, :].dot(self.item_vecs[i, :].T)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_learning_curve(self, test_matrix):\n",
    "        \"\"\"\n",
    "        Track MSE as a function of training iterations.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        test : (2D ndarray)\n",
    "            Testing dataset (assumed to be USER x ITEM).\n",
    "        \n",
    "        The function creates two new class attributes:\n",
    "        \n",
    "        train_mse : (list)\n",
    "            Training data MSE values for each value of iterations\n",
    "        test_mse : (list)\n",
    "            Test data MSE values for each value of iterations\n",
    "        \"\"\"\n",
    "\n",
    "        print (\"Calculate learning curve\")\n",
    "\n",
    "        self.iterations.sort()\n",
    "        self.train_mse = []\n",
    "        self.test_mse  = []\n",
    "        iter_diff = 0\n",
    "\n",
    "        for (i, n_iter) in enumerate(self.iterations):\n",
    "            print ('{}, {}'.format(i, n_iter))\n",
    "            if self._v:\n",
    "                print('Iteration: {}'.format(n_iter))\n",
    "            if i == 0:\n",
    "                print('i = 0; train({})'.format(n_iter - iter_diff))\n",
    "                self.train(n_iter - iter_diff)\n",
    "            else:\n",
    "                print('partial_train({})'.format(n_iter - iter_diff))\n",
    "                self.partial_train(n_iter - iter_diff)\n",
    "\n",
    "            predictions = self.predict_all()\n",
    "\n",
    "            self.train_mse += [get_mse(predictions, self.ratings)]\n",
    "            self.test_mse  += [get_mse(predictions, test_matrix)]\n",
    "            if (self._v):\n",
    "                print('Train mse: ' + str(self.train_mse[-1]))\n",
    "                print('Test mse:  ' + str(self.test_mse[-1]))\n",
    "            iter_diff = n_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def get_mse(pred, actual):\n",
    "    # Ignore nonzero terms.\n",
    "    pred   = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return mean_squared_error(pred, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iter_array = [1, 2, 5, 10, 15, 25, 50, 75, 100]\n",
    "MF_ALS = ExplicitMF(ratings_matrix, n_factors=40, user_reg=0.0, item_reg=0.0, iterations=iter_array, verbose=True)\n",
    "MF_ALS.calculate_learning_curve(testing_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model):\n",
    "    # create our data traces (training MSE and testing MSE)\n",
    "    trace_training = go.Scatter(x=model.iterations, y=model.train_mse, name='training')\n",
    "    trace_testing  = go.Scatter(x=model.iterations, y=model.test_mse,  name='testing')\n",
    "    layout = dict(\n",
    "        title=\"MovieLens Learning Curve\", \n",
    "        xaxis=dict(title=\"Iterations\"),\n",
    "        yaxis=dict(title=\"Mean Squared Error\")\n",
    "    )\n",
    "\n",
    "    fig  = dict(data=[trace_training, trace_testing], layout=layout)\n",
    "    py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(MF_ALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
